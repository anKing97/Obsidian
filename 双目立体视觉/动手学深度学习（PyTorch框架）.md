# 动手学深度学习（PyTorch框架）

Created: April 16, 2022 10:43 AM
Reviewed: No

# 阅读指南：

• 第一部分（第1章至第3章）涵盖预备工作和基础知识。第1章介绍深度学习的背景。第2章提供动手学深度学习所需要的预备知识。第3章包括深度学习最基础的概念和技术，如多层感知机和模型正则化。如果读者时间有限，并且只想了解深度学习最基础的概念和技术，那么只需阅读第一部分。

• 第二部分（第4章至第6章）关注现代深度学习技术。第4章描述深度学习计算的各个重要组成部分，并为实现后续更复杂的模型打下基础。第5章解释近年来令深度学习在计算机视觉领域大获成功的卷积神经网络。第6章阐述近年来常用于处理序列数据的循环神经网络。阅读第二部分有助于掌握现代深度学习技术。

• 第三部分（第7章至第10章）讨论计算性能和应用。第7章评价各种用来训练深度学习模型的优化算法。第8章检验影响深度学习计算性能的几个重要因素。第9章和第10章分别列举深度学习在计算机视觉和自然语言处理中的重要应用。这部分内容读者可根据兴趣选择阅读。

![Untitled](attachments/Untitled%206.png)

上图中由甲章指向乙章的箭头表明甲章的知识有助于理解乙章的内容。如果读者想短时间了解深度学习最基础的概念和技术，只需阅读第1章至第3章；如果读者希望掌握现代深度学习技术，还需阅读第4章至第6章。第7章至第10章读者可以根据兴趣选择阅读。

# 深度学习简介

逆向思考，与其涉及一个解决问题的程序，不如从嘴中的需求入手来找寻一个解决方案。即不用去想如何解决问题，而是以人的观点去输入一些数据，让计算机学习。

通俗来说，机器学习是一门讨论各式各样的适用于不同问题的函数形式，以及如何使用数据来有效地获取函数参数具体值的学科。深度学习是指机器学习中的一类函数，它们的形式通常为多层神经网络。

# 第一章：前言

训练过程通常包含如下步骤：

1.  从一个随机初始化参数的模型开始，这个模型基本毫不“智能”。
2. 获取一些数据样本（比如：音频片段以及对应的{是，否}标签）。
3. 调整参数，使模型在这些样本中表现得更好。
4. 重复第2步和第3步，直到模型在任务中的表现令人满意。

![Untitled](attachments/Untitled%201%202.png)

## 1.2. 关键组件

1. 我们可以学习的 ***数据*（data）**
2. 如何转换数据的 ***模型*（model）**
3. 一个 ***目标函数*（objective function）**， 用来量化模型的有效性。
4. 调整模型参数以优化目标函数的 *算法***（algorithm）**。

### 1.2.1.数据

每个数据集由一个个 ***样本（sample）*** 组成，大多时候，他们遵循独立同分布（independently and identically distributed）。样本有时候也叫做 ***数据点（data point）*** 或者 ***数据实例（data instance）*** 。

当每个样本的特征类别数量都是相同的时候，其特征向量是固定长度的，这个长度被称为数据的 ***维数（dimensionality）*** 。

> 输入的是垃圾，输出的也是垃圾 （Garbage in ，Garbage out）
> 

如果数据不具有充分的代表性，甚至包含了一些社会的偏见时，那么模型就很有可能有偏见。

### 1.2.2.模型

深度学习与经典方法的区别主要在于：前者关注的功能强大的模型，这些模型由神经网络错综复杂的交织在一起，包含层层数据转换，因此被称为 ***深度学习*** （deep learning）。

### 1.2.3.目标函数

对模型优略程度的度量，这个度量在大多数情况下是可优化的，我们称之为 ***目标函数*** （objective function）。希望将这个函数优化至最低点，因为越低越好，所以这些函数有时候也被称为 ***损失函数*** （loss function）。 常见的损失函数是 ***平方误差*** （squared error），即预测值与实际值只差的平方。

可以将数据集分为两部分：***训练数据集 ：*** 用于拟合模型参数，***测试数据集：***用于评估拟合的模型。

当一个模型在训练集上表现良好，但不能推广到测试集时，我们说这个模型是 ***过拟合*** （overfitting）。

### 1.2.4.优化算法

用于搜索出最佳参数，以最小化损失函数，深度学习中，大多流行的优化算法通常基于一种基本方法—— ***梯度下降*  （gradient descent）。**在每个步骤中，梯度下降法都会检查每个参数，看看如果你仅对该参数进行少量变动，训练集损失会朝哪个方向移动。然后，他在可以减少损失的方向上优化参数。

## 1.3.各种机器学习问题

### 1.3.1.监督学习

***监督学习*** （supervised learning）擅长在“给定输入特征”的情况下预测标签。每个 “特征-标签”都称为一个样本。目的是生成一个模型，能够将任何输入特征映射到标签，即预测。

监督学习的过程为：首先，从已知大量数据样本中随机选取一个子集，为每个样本获取基本的真实标签。这些输入和相应的标签一起构成了训练数据集。随后，利用有监督学习算法，将训练数据集作为输入，并输出一个“完成学习模型”。之后用于测试集，评测这个模型。

> 定量输出称为 **回归** ，或者说是连续变量预测。
定性输出称为 **分类** ，或者说是离散变量预测。
> 
1. ***回归***
    
    从一组数据出发，确定某些变量之间的定量关系式；即建立数学模型并估计未知参数。解决的是“有多少”的问题。
    
    **训练对象是**：回归函数
    
2. ***分类***
    
    从一组数据出发，预测样本属于哪个类别。解决的是“哪一个”的问题。
    
    **训练对象是：**分类器
    
3. ***标记问题***
    
    学习预测不相互排斥的类别问题称为 ***多标签分类*** （multi-label classification）。
    
4. ***搜索***
5. ***推荐系统***
6. ***序列学习***

### 1.3.2.无监督学习

数据中不含有“目标”的机器学习问题为 ***无监督学习*** （unsupervised learning）

- ***聚类*** （clustering）问题：在没有标签的情况下给数据分类。
- ***主成分分析*** （principal component analysis）问题：找到少量的参数来准确的捕捉数据的线性相关属性。
- *因果关系* 和 *概率图模型* 问题：
- ***生成对抗行网络 ：***

# 第2章：预备知识

## 2.1.数据操作

类似于 **NumPy** 中的 **ndarray** ，在 **Pytorch** 中称为 ***张量*** （tensor）

### 2.1.1.入门

首先导入torch包： `import torch` 

`torch.arange` ：创建张量数组。 `x.shape` ：访问张量的形状。

`x.reshape` ：改变张量的形状，可以输入 `-1` 来自动计算维度。

`torch.zeros` ：创建全0张量。 `torch.ones` ：创建全1张量。

`torch.randn` ：从均值为0、标准差为1的标准高斯分布中随机采样。

`torch.tensor` ：将Python内置列标转换为对应的张量。

### 2.1.2.运算符

常见的标准运算符： +、-、*、/ 和 **  全部被转化为按元素运算。

利用 `.cat((X,Y),dim=0)` 可以将两个矩阵连接在一起。

### 2.1.3.广播机制

当形状不同时，可以调用广播机制来进行按元素操作。工作方式如下：首先，通过适当复制元素来扩展一个或两个数组，以便在转换之后，两个张量具有相同的形状。其次，对生成的数组执行按元素操作。

### 2.1.4.索引和切片

`0` 为第一个元素， `-1` 为最后一个元素

### 2.1.5.节省内存

### 2.1.6.转换为其他Python对象

`A = X.numpy()`  `B=torch.tensor(A)`

## 2.2.数据预处理

### 2.2.1.读取数据

调用 **Pandas** 读取 csv 文件： `pandas.read_csv()` 

### 2.2.2.处理缺失值

典型的方法为 ***插值法*** 和 ***删除法*** 。 `fillna()` 函数

### 2.2.3.转换为张量格式

`torch.tensor`

## 2.3.线性代数

`torch.mv(A,x)` : 计算矩阵-向量积。

`torch.mm(A,B)` ：计算矩阵-矩阵积

### 范数

一个向量的范数告诉我们一个向量有多大。这里的大小不涉及维度，而是分量的大小。

在线性代数中，向量范数是将向量映射到标量的函数$f$。 给定任意向量x，向量范数要满足一些属性。 第一个性质是：如果我们按常数因子$\alpha$缩放向量的所有元素， 其范数也会按相同常数因子的*绝对值*缩放：$f(\alpha x)=|\alpha|f(x)$ 
第二个性质时三角不等式：$f(x+y)\le f(x)+f(y)$
第三个性质为非负性：$f(x)\ge 0$

`torch.norm(x)` 计算向量的 $L_2$ 范数

`torch.abs(u).sum()` 计算向量的$L_1$ 范数。

## 2.4.微积分

### 2.4.3.梯度

可以连结一个多元函数对其所有变量的偏导数，以得到该函数的 ***梯度*** 向量。函数 $f(x)$相对于$x$的梯度是一个包含$n$个偏导数的向量：

$\bigtriangledown_x f(x)=[{\frac{\partial f(x)}{\partial x_1},\frac{\partial f(x)}{\partial x_2},...,\frac{\partial f(x)}{\partial x_n} }]^T$ 

假设$x$为$n$维向量，在微分多元函数时经常使用以下规则：

- 对于所有的$\mathrm{A} \in \mathbb{R}^{m\times n}$，都有$\bigtriangledown _x \mathrm{Ax}=\mathrm{A}^T$
- 对于所有的$\mathrm{A} \in \mathbb{R}^{n\times m}$，都有$\bigtriangledown _x \mathrm{x}^T\mathrm{A}=\mathrm{A}$
- 对于所有的$\mathrm{A} \in \mathbb{R}^{n\times n}$，都有$\bigtriangledown _x \mathrm{x}^T\mathrm{Ax}=(\mathrm{A+A^T})\mathrm{x}$
- $\bigtriangledown _x||\mathrm{x}||^2 = \bigtriangledown _x\mathrm{x^Tx}=2\mathrm{x}$

## 2.5.自动微分

深度学习框架通过自动计算导数，即*自动微分*（automatic differentiation）来加快求导。 实际中，根据我们设计的模型，系统会构建一个*计算图*（computational graph）， 来跟踪计算是哪些数据通过哪些操作组合起来产生输出。 自动微分使系统能够随后反向传播梯度。 这里，*反向传播*（backpropagate）意味着跟踪整个计算图，填充关于每个参数的偏导数。

利用***反向传播函数*** 来计算微分： `y.backward()`  输出 `x.grad`

## 2.6.概率

# 第3章：线性神经网络

## 3.1.线性回归

常见的例子：预测价格、预测住院时间、预测需求等

### 3.1.1.线性回归的基本元素

***线性回归*** ：假设自变量 $x$ 和因变量 $y$ 之间的关系是线性的，即 $y$  可以表示为 $x$ 中元素的加权和。这里通常允许包含观测值的一些噪声；其次，假设任何噪声都是比较正常的，如噪声遵循正态分布。

1. ***线性模型***：
    
    如：$price=\omega_{area}·area+\omega_{age}·age+b$
    
    其中 $\omega_{area}和\omega_{age}$ 称为权重，权重决定了每个特征对预测值的影响， $b$ 称为 ***偏置***、***偏移量*** 或 ***截距*** 。偏置是指：所有的特征都取0时，预测值应为多少。
    
    给定一个数据集，目标即为寻找权重 $\omega$ 和偏置 $b$ 。
    
    在机器学习领域，通常是好几个特征：$\hat{y}=\omega_1·x_1+...+\omega_d·x_d+b$ 
    
2. ***损失函数***：
    
    用于度量模型质量的方式。能够量化目标的实际值和预测值之间的差距。
    
    常用的损失函数是：平方误差函数
    
3. ***解析解***：
    
    线性回归的解可以用一个公式简单地表达出来。这类解叫做解析解。
    
4. ***随机梯度下降***：
    
    ***梯度下降法*** 几乎可以优化所有的深度学习模型。通过不断地在损失函数递减的方向上更新参数来降低误差。
    
    最简单的用法是计算损失函数关于模型参数的导数。这种做法可能会很慢，因此，通常会在每次需要计算更新的时候随机抽取一小批样本，称为 ***小批量随机梯度下降***。
    
    **算法的步骤为**：
    
    1. 初始化模型参数的值，如随机初始化；
    2. 从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。

### 3.1.2.矢量化加速

同时处理整个小批量的样本。不用按元素级运算，直接对两个向量变量名进行加减等运算操作。

### 3.1.3.正态分布与平方损失

正态分布：也称为 高斯分布。 简单说：若随机变量 $x$ 具有均值 $\mu$ 和方差 $\sigma^2$ （标准差为 $\sigma$），其正态分布概率密度函数如下：$p(x)=\frac{1}{\sqrt{2\pi \sigma ^2} } (-\frac{1}{2\sigma ^2} (x-\mu )^2)$

### 3.1.4.从线性回归到深度网络

1. ***神经网络图***：
    
    ![Untitled](attachments/Untitled%202%201.png)
    
    通常会忽略网络的输入层，因此，这个模型为单层神经网络。
    
    对于线性回归，每个输入都与每个输出相连，我们将这种变换称为 ***全连接层*** 或 ***稠密层***。
    

## 3.2.线性回归的从零开始实现

## 3.3.线性回归的简洁实现

## 3.4.softmax回归

### 3.4.1.分类问题

***独热编码***： 表示分类数据的简单方法，它是一个向量，分量和类别一样多，类别对应的分量设置为1，其他所有分量设置为0。

### 3.4.2.网络架构

![Untitled](attachments/Untitled%203%201.png)

与线性回归一样，softmax回归也是一个单层神经网络。并且其输出也是全连接层。

### 3.4.3.全连接层的参数开销

对于任何具有 $d$ 个输入和 $q$ 个输出的全连接层，参数开销为： $\Theta(dq)$ ，可以减小成本：将$d$ 个输入转换为 $q$ 个输出的成本可以减少到 $\Theta(\frac{dq}{n} )$。

### 3.4.4.softmax运算

需要一个训练目标，来鼓励模型精准地估计概率。在分类器输出0.5的所有样本中，希望这些样本有一半shi'jia

实现softmax的步骤：

1. 对每个项求幂（使用exp）；
2. 对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数；
3. 将每一行除以其规范化常数，确保结果的和为1.
    
    $softmax(X)_{ij}=\frac{exp(X_{ij})}{\sum_kexp(X_{ik})}$  
    

### 交叉熵损失函数

[损失函数｜交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)

1. ***表达式***：
    1. **二分类**
        
        在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为 $p$ 和 $1-p$ 。此时表达式为：
        
        $L=\frac{1}{N} \sum _iL_i=\frac{1}{N} \sum _i-[y_i·log(p_i)+(1-y_i)·log(1-p_i)]$ 
        
        其中：
        
        $y_i$ ——表示样本 $i$ 的label，正类为1，负类为0
        
        $p_i$ ——表示样本  $i$ 预测为正类的概率。 
    2. **多分类**
        
        $L =\frac{1}{N} \sum _iL_i=-\frac{1}{N} \sum _i\sum _{c=1}^My_{ic}log(p_{ic})$ 
        
        其中：
        
        $M$ ——类别的数量
        
        $y_{ic}$ ——符号函数（0或1）
        
        $p_{ic}$  ——观测样本  $i$ 属于类别 $c$ 的预测概率


## 3.5.图像分类数据集

***MNIST数据集*** 是图像分类中广泛使用的数据集之一。 还有 ***Fashion-MNIST数据集***。

# 第4章：多层感知机

## 4.1.多层感知机

### 4.1.4.隐藏层

1. ***线性模型可能会出错***
    
    you
    
2. ***在网络中加入隐藏层***：
    
    可以在输入与输出之间加入一个或几个隐藏层：将许多全连接层堆叠在一起。每一层都输出到上面的层，直到生成最后的输出。
    
    ***多层感知机***： 将前 $L-1$ 层看作表示，把最后一层看作线性预测器。
    
    ![Untitled](attachments/Untitled%204%201.png)
    
    这个多层感知机有4个输入，3个输出，其隐藏层包含了5个隐藏单元。其层数为2层，且都是全连接层。
    
3. ***从线性到非线性***：
    
    $\mathrm {H}^{(1)} =\sigma _1(\mathrm{XW^{(1)}+b^{(1)})}$  和  $\mathrm {H}^{(2)} =\sigma _2(\mathrm{H^{(1)}W^{(2)}+b^{(2)})}$ 
    
4. ***通用近似定理***：
    
    多层感知机可以通过隐藏神经元，捕捉到输入之间复杂的相互作用，这些神经元依赖于每个输入的值。
    

### 4.1.2.激活函数

***激活函数*** 通过计算加权和并加上偏置来确定神经元是否应该被激活，他们将输入信号转换为输出的可微运算。

1. ***ReLU函数***：
    
    ***修正线性单元（ReLU）*** ：该元素与 0 的最大值。$ReLU(x)=max(x,0)$  
    
    通过将相应的活性值设为 0 ，仅保留正元素并丢弃所有的负元素。
    
    当输入为负时，ReLU函数的导数为0，而当输入为正时，ReLU函数的导数为1。
    
    ReLU函数的有点：求导表现特别好：要么让参数消失，要么让参数通过。并且减轻了困扰神经网络的梯度消失问题。
    
2. ***sigmoid函数***：
    
    将实数 $\mathbb{R}$ 上的输入转变为 $（0，1）$上的输出，也称为 ***挤压函数***。
    
    $sigmoid(x)=\frac{1}{1+exp(-x)}$  
    
    常用用途：利用 sigmoid单元 来控制时序信息流的架构
    
3. ***tanh函数***
    
    同样是压缩到 $（-1，1）$上：$tanh(x)=\frac{1-exp(-2x)}{1+exp(-2x)}$  
    

## 4.2.模型选择、欠拟合和过拟合

将模型在训练数据上拟合的比潜在分布中更接近的现象称为 ***过拟合***。 用于对抗过拟合的技术称为 ***正则化**。 

### 4.2.1.训练误差和泛化误差

***训练误差：*** 模型在训练数据集上得到的误差。

***泛化误差：*** 模型应用在同样从原始样本的分布中抽取的无限多数据样本时，模型误差的期望。

1. **统计学习理论**：
2. **模型复杂性**

### 4.2.2.模型选择

1. ***验证集***
    
    将数据集分为三份，训练集、验证集和测试集。但是验证集和测试集之间的边界却很模糊。
    
2. ***K折交叉验证***
    
    原始训练数据被分成 K 个不重叠的子集。然后执行 K 次模型训练和验证，每次在 K-1 个子集上进行训练，并在剩余的一个子集上进行验证。最后，通过对 K 次实验结果取平均值来估计训练和验证误差。
    

## 4.3.权重衰减

正则化模型的技术。在训练集的损失函数中加入惩罚项，以降低学习到的模型的复杂度

### 4.3.1.范数与权重衰减

$L(w,b)=\frac{1}{n} \sum _{i=1}^n\frac{1}{2} (w^Tx^{(i)}+b-y^{(i)})^2$。 

$L(w,b)+\frac{(\lambda )}{2} ||w||^2$。  

$w\gets (1-\eta \lambda )w-\frac{\eta }{\mathcal{B} } \sum _{i\in \mathcal{B} }x^{(i)}(w^Tx^{(i)}+b-y^{(i)})$  

## 4.4.暂退法

## 4.5.前向传播、反向传播和计算图

## 4.6.数值稳定性和模型初始化

### 4.6.1.梯度消失和梯度爆炸

# 第6章：卷积神经网络（CNN）

## 6.1.从全连接层到卷积

### 6.1.1.不变性

合理的假设：无论哪种方法找到这个物体，都应该和物体的位置无关。 ***卷积神经网络*** 正是将 空间不变性 这一概念系统化。

适合于计算机视觉的神经网络架构：

1. ***平移不变性***： 不管检测对象出现在图像的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应。
2. ***局部性：*** 神经网络前面几层应该只探索输入图像的局部区域，而不过度在意图像中相隔较远区域的关系。

### 6.1.2.多层感知机的限制

多层感知机的输入是二维图像 $\mathrm{X}$ ，其隐藏表示 $\mathrm{H}$ 在数学上是一个矩阵，在代码中表示为二维张量。

$[\mathrm{H}_{i,j}] = [\mathrm{U}]_{i,j} +\sum _k \sum _l  [W]_{i,j,k,l}[\mathrm{X}] _{k,l} \\ =[\mathrm{U}]_{i,j} + \sum _a^k\sum _b^l[\mathrm{V}]_{i,j,a,b}[\mathrm{X} ]_{i+a,j+b}$

1. **平移不变性：** 意味着检测对象在输入 $\mathrm{X}$ 中的平移，应该仅导致隐藏表示 $\mathrm{H}$ 中的平移。即 $U$ 和 $V$ 实际上不依赖于 $（i，j）$的值，即 $[V]_{i,j,a,b}=[V]_{a,b}$ ，并且 $U$ 是一个常数，此时，我们可以简化 $H$  的定义为：
    
    $[\mathrm{H}]_{i,j}=u+\sum _a\sum _b[V]_{a,b}[X]_{i+a.j+b}$  
    
    这就是 *卷积* 
    
2. **局部性**
    
    在距离 $（i，j）$很远的地方，可以设置 $[V]_{a,b}=0$ 。因此，可以重写为：
    
    $[\mathrm{H}]_{i,j}=u+\sum _a^\bigtriangleup \sum _b^\bigtriangleup [V]_{a,b}[X]_{i+a.j+b}$  
    
    此为一个卷积层， $\mathrm{V}$ 被称为 *卷积核*  或者 *滤波器  ，*或者称为该*卷积层的权重。* 
    

### 6.1.3.卷积

卷积是当把一个函数“翻转”并位移 $x$ 时，测量 $f和g$ 之间的重叠。

### 6.1.4.“沃尔多在哪里”

1. ***通道：***
    
    一般图像包含三个通道/三种原色（红、绿、蓝），而且图像是由高度、宽度和颜色组成的三维张量。此时将 $X$ 索引为： $[X]_{i,j,k}$ ，卷积相应调整为： $[V]_{a,b,c}$ . 
    

## 6.2.图像卷积

### 6.2.1.互相关运算

### 6.2.2.卷积层

卷积层对输入和卷积核权重进行互相关运算，并在添加标量偏置后产生输出。所以卷积层中的两个被训练的参数就是卷积核权重和标量偏置。

对图像进行边缘检测：利用卷积核找出相邻元素之间不一样的部分，即为边缘。

### 6.2.4.学习卷积核

### 6.2.5.互相关和卷积

## 6.3.填充和步幅

填充卷积图像，防止原图像经过卷积之后图像尺寸变小。

而步幅则是为了减小图像的大小，因为有时候图像的分辨率十分冗余，需要进行缩小。

### 6.3.1.填充

在许多情况下，我们需要设置 $p_h=k_h-1 和p_w=k_w-1$ ，使输入和输出具有相同的高度和宽度。假设 $k_h$ 使奇数，我们将在高度的两侧填充 $\frac{p_h}{2}$ 行，如果 $k_h$ 使偶数，则一种可能性是在输入的顶部和底部填充 $\frac{p_h}{2}$行。

卷积神经网络中的卷积核的高度和宽度通常为奇数。

### 6.3.2.步幅

通常情况下，卷积窗口在输入张量的左上角开始，向下、向右滑动，每次滑动一个元素。但是，为了高效计算或是缩减采样次数，卷积窗口可以跳过中间位置，每次滑动多个元素。

通常，当垂直步幅为 $s_h$、水平步幅为 $s_w$ 时，其输出形状为：

$\lfloor (n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor (n_w-k_w+p_w+s_w)/s_w\rfloor$ 

## 6.4.多输入多输出通道

### 6.4.1.多输入通道

当输入包含多个通道时，需要构造一个与输入数据具有相同输入通道数的卷积核，以便于输入数据进行互相关运算。

基本上，我们将每个通道输入的二维张量和卷积核的二维张量进行互相关运算，再对通道求和得到二维张量。

### 6.4.2.多输出通道

由于上面得到的是一个二维张量，但是多通道的输出又是至关重要的，在最流行的神经网络架构中，随着神经网络层数的加深，我们通常会增加通道的维数，通过减少空间分辨率以获得更大的通道深度。只管来说：我们可以将每个通道看作是对不同特征的响应。

为了获得多通道的输出，我们可以为每个输出通道创建一个与输入通道数相同，与卷积核大小相同的卷积核张量，在互相关运算中，每个输出通道先获取所有输入通道，再以对应该输出通道的卷积核计算结果。

## 6.5.汇聚层

***汇聚层的目的：*** 降低卷积层对位置的敏感性；同时降低对空间采样表示的敏感性。

### 6.5.1.最大汇聚层和平均汇聚层

汇聚层与卷积层类似，但是不涉及互相关运算，而是计算汇聚窗口中所有元素的最大值或平均值，其分别被称为： ***最大汇聚层*** 和 ***平均汇聚层***。

汇聚窗口形状为 $p\times q$ 的汇聚层称为 $p\times q$ 汇聚层，汇聚操作称为 $p\times q$ 汇聚。

### 6.5.2.填充和步幅

和卷积层一样，汇聚层同样可以进行填充以及更改步幅。

### 6.5.3.多个通道

在处理多通道输入数据时，汇聚层在每个输入通道上单独运算，而不是像卷积层一样在通道上对输入进行汇总。这意味着汇聚层的输出通道数与输入通道数相同。

## 6.6.卷积神经网络（LeNet）

### 6.6.1.LeNet

总体来看，LeNet由两个部分组成：

- 卷积编码器：由两个卷积层组成
- 全连接层密集块：由三个全连接层组成。

![Untitled](attachments/Untitled%205%201.png)

每个卷积快中的基本单元是一个卷积层、一个sigmoid激活函数和平均汇聚层。

# 第8章：循环神经网络

## 8.1.序列模型

### 8.1.1.统计工具

1. ***自回归模型***：
    1. 通过截取固定长度的时间序列来观测数值，因此，参数的数量总是不变的，这种模型被称为 **自回归模型** ，因为他们是对自己执行回归。
    2. 保留一些对过去观测的总结，并且同时更新预测和总结，在此过程中，有一部分从未被观测到，因此，这个模型被称为 **隐变量自回归模型**。
2. ***马尔可夫模型***：
3. ***因果关系：***

## 8.2.文本预处理

通常的步骤为：

1. 将文本作为字符串加载到内存中
2. 将字符串拆分为词元（如单词和字符）
3. 建立一个词表，将拆分的词元映射到数字索引
4. 将文本转换为数字索引序列，方便模型操作。

## 8.3.语言模型和数据集

### 8.3.1.学习语言模型
/git